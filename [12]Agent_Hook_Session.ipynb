{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdVTT9Oy97HkafQuczuw1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalRashid9775/AI_Agents/blob/main/%5B12%5DAgent_Hook_Session.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** LifeCycles **  \n",
        "There are two types of loops. Agent Loops and Run Loops.\n",
        "\n",
        "\n",
        "**Agent lifecycle hook**:  Whenever a user ask a query to agent. This query will send to LLM from agent and LLM process this query and check whether it answer this query by itself or call a tool.\n",
        "\n",
        "**Run Lifecycle**:  If LLM call a tool then its response send to LLM and then LLM send final response to user. You can show these whole process to user. For this we use lifecycle hook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tTV1LpqagNA3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS3IFg1cYkOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7538af1d-b0e2-42af-fbea-2770fb10c6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/210.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agent Lifecycle**\n",
        "\n",
        "**On_start** : this event/hook fire before activation of agent.\n",
        "\n",
        "\n",
        "**On_end** : this event/hook fire at the end of agent working or when agent get final_output.\n",
        "\n",
        "\n",
        "**On_llm_start** : this hook start when llm processing start.\n",
        "\n",
        "\n",
        "**On_llm_end** : this hook start when llm processing end.\n",
        "\n",
        "\n",
        "**On_tool_start**:  before start  execution of tool.\n",
        "\n",
        "\n",
        "**On_tool_end**: after end of execution of tool.\n",
        "\n",
        "\n",
        "**On_handoff** : when handoffs\n"
      ],
      "metadata": {
        "id": "h4RPF4jGhHpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from agents import Agent,Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled, function_tool,AgentHooks,RunContextWrapper\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "base_url = userdata.get('BASE_URL')\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model='gemini-2.5-flash',\n",
        "    openai_client=client\n",
        ")\n",
        "#Pass this class to agent hooks\n",
        "class weather_hooks(AgentHooks):\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "\n",
        "  async def on_start(self, context: RunContextWrapper, agent:Agent) -> None:\n",
        "     print(f'Agent {agent.name} started')\n",
        "\n",
        "  async def on_llm_start(self, context: RunContextWrapper, agent, system_prompt, input_items) -> None:\n",
        "     # Access the 'text' field of the first item in the input_items list\n",
        "     print(f'LLM Start working on {input_items} started')\n",
        "  async def on_llm_end(self, context: RunContextWrapper, agent, output) -> None:\n",
        "     print(f'Agent {output} ended')\n",
        "  async def on_tool_start(self, context: RunContextWrapper, agent, tool) -> None:\n",
        "     print(f'Agent {tool.name} started')\n",
        "  async def on_tool_end(self, context: RunContextWrapper, agent, tool, output) -> None:\n",
        "     print(f'Tool Response :  {output} ended')\n",
        "  async def on_end(self, context: RunContextWrapper, agent, output) -> None:\n",
        "     print(f'Agent output :  {output}')\n",
        "@function_tool()\n",
        "async def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    return f'The weather for {location} is sunny and temperature is 25C'\n",
        "set_tracing_disabled(disabled=True)\n",
        "weather_agent = Agent(\n",
        "    name='weather_agent',\n",
        "    instructions='You are helpful agent',\n",
        "    model=model,\n",
        "    tools=[get_current_weather],\n",
        "    hooks=weather_hooks('weather_agent')\n",
        "    )\n",
        "response = Runner.run_sync(weather_agent,'How the weather in Multan')\n",
        "print(response.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2mhS3e9Yz6T",
        "outputId": "060d5053-b254-4722-ffe8-e080b2561095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent weather_agent started\n",
            "LLM Start working on [{'content': 'How the weather in Multan', 'role': 'user'}] started\n",
            "Agent ModelResponse(output=[ResponseFunctionToolCall(arguments='{\"location\":\"Multan\"}', call_id='', name='get_current_weather', type='function_call', id='__fake_id__', status=None)], usage=Usage(requests=1, input_tokens=54, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=18, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=135), response_id=None) ended\n",
            "Agent get_current_weather started\n",
            "Tool Response :  The weather for Multan is sunny and temperature is 25C ended\n",
            "LLM Start working on [{'content': 'How the weather in Multan', 'role': 'user'}, {'arguments': '{\"location\":\"Multan\"}', 'call_id': '', 'name': 'get_current_weather', 'type': 'function_call', 'id': '__fake_id__'}, {'call_id': '', 'output': 'The weather for Multan is sunny and temperature is 25C', 'type': 'function_call_output'}] started\n",
            "Agent ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The weather in Multan is sunny with a temperature of 25°C.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=103, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=120), response_id=None) ended\n",
            "Agent output :  The weather in Multan is sunny with a temperature of 25°C.\n",
            "The weather in Multan is sunny with a temperature of 25°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Lifecycle**\n",
        "\n",
        "Run lifecycle: Agent hooks only work for a particular events but run lifecyle would works for whole of the loop.\n",
        "\n",
        "\n",
        "**On_start_agent** : any agent become active.\n",
        "\n",
        "\n",
        "**On_agent_end** : any agent end.\n",
        "\n",
        "\n",
        "**On_llm_start** : when llm start.\n",
        "\n",
        "\n",
        "**On_llm_end** : when llm end.\n",
        "\n",
        "\n",
        "**On_tool_start**: when any tool start.\n",
        "\n",
        "\n",
        "**On_tool_end** : when any tool end.\n",
        "\n",
        "\n",
        "**On_hand0ff** : when any agents handoff.\n",
        "\n",
        "\n",
        "For this we run hooks on runner:\n"
      ],
      "metadata": {
        "id": "khjF8t0pkCee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chat-Bots**"
      ],
      "metadata": {
        "id": "36qIy9bzlQHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from agents import Agent,Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled, function_tool,AgentHooks,RunContextWrapper\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "base_url = userdata.get('BASE_URL')\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model='gemini-2.5-flash',\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "@function_tool()\n",
        "async def get_current_weathers(location: str) -> str:\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    return f'The weather for {location} is sunny and temperature is 25C'\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "weather_agent = Agent(name='weather_agent', instructions='You are helpful agent', model=model,tools=[get_current_weathers])\n",
        "user_chat=[]\n",
        "while True:\n",
        "  user_input = input(\"Enter your message: \")\n",
        "  if user_input.lower()=='exit':\n",
        "    break\n",
        "  user_chat.append({'role':'user','content':user_input})\n",
        "  response = Runner.run_sync(weather_agent,user_chat)\n",
        "  print(response.final_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6UpKb7Cy7FQ",
        "outputId": "4e3ad15e-e760-4b0c-f263-8ef750930391"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your message: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Session**"
      ],
      "metadata": {
        "id": "ydT67HhH3MVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from agents import Agent,Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled, function_tool,AgentHooks,RunContextWrapper, SQLiteSession\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "base_url = userdata.get('BASE_URL')\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model='gemini-2.5-flash',\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "session  = SQLiteSession('user123', 'user.db')\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "weather_agent = Agent(name='weather_agent', instructions='You are helpful agent', model=model)\n",
        "response = Runner.run_sync(weather_agent,'what is my name', session=session)\n",
        "print(response.final_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNReS433zUqt",
        "outputId": "5aefcb10-ea52-4698-85e7-83b6a26ab029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know your name. As an AI, I don't have access to your personal information.\n",
            "\n",
            "You'd have to tell me! What would you like me to call you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Memory**"
      ],
      "metadata": {
        "id": "sI2JBMjCVY59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -Uq mem0ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVDyo4ogVctD",
        "outputId": "32e0212d-aa6b-46f6-a3dd-7f7c76e5d6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m235.5/239.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/337.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from agents import Agent,Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled,function_tool\n",
        "from google.colab import userdata\n",
        "from mem0 import MemoryClient\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "base_url = userdata.get('BASE_URL')\n",
        "mem0_api_key = userdata.get('MEM0')\n",
        "\n",
        "mem_client = MemoryClient(api_key=mem0_api_key)\n",
        "\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model='gemini-2.5-flash',\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "@function_tool()\n",
        "async def get_Search_memory(query:str):\n",
        "  \"\"\"Search memory\"\"\"\n",
        "  print('Searching query')\n",
        "  return  mem_client.search(query=query,user_id = 'poo1',top_k=3)\n",
        "@function_tool()\n",
        "async def saving_memory(query:str):\n",
        "  \"\"\"Saving memory\"\"\"\n",
        "  print('Saving memory')\n",
        "  return mem_client.add([{'role':'user', 'content':query}],user_id = 'poo1')\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "weather_agent = Agent(name='weather_agent', instructions='You are helpful agent use tool get_search_memory for getting info', model=model,tools=[get_Search_memory,saving_memory])\n",
        "response = Runner.run_sync(weather_agent,'What my name')\n",
        "print(response.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucSEbeCzVuOx",
        "outputId": "c844d5eb-a78d-44da-ab1a-3a21cd3afcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching query\n",
            "Your name is Ali.\n"
          ]
        }
      ]
    }
  ]
}